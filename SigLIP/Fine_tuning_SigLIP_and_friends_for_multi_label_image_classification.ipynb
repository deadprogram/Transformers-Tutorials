{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/deadprogram/Transformers-Tutorials/blob/master/SigLIP/Fine_tuning_SigLIP_and_friends_for_multi_label_image_classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fine-tune SigLIP and friends for multi-label image classification\n",
        "\n",
        "In this notebook, I'll show how one can fine-tune any pre-trained vision model from the Transformers library for multi-label image classification.\n",
        "\n",
        "It's the computer vision counterpart of a [similar notebook](https://github.com/NielsRogge/Transformers-Tutorials/blob/master/BERT/Fine_tuning_BERT_(and_friends)_for_multi_label_text_classification.ipynb) I made for multi-label text classification with BERT and friends.\n",
        "\n",
        "As we'll see, the API is pretty identical, the only difference is that we now need to prepare images (`pixel_values`) for the model rather than text (`input_ids` and `attention_mask`). As every vision model in the library has the same API, one can plug in any pre-trained image encoder from the [hub](https://huggingface.co/). Here we'll use [SigLIP](https://huggingface.co/docs/transformers/en/model_doc/siglip) as it's one of the strongest vision encoders at the time of writing.\n",
        "\n",
        "Note: you'll need to run this notebook on a GPU.\n",
        "\n",
        "## Set-up environment\n",
        "\n",
        "Let's start by installing ðŸ¤— Transformers and ðŸ¤— Datasets."
      ],
      "metadata": {
        "id": "lh7XQryIbpwC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# we're installing from source since the model we will be using is brand new at the time of writing\n",
        "!pip install --upgrade git+https://github.com/huggingface/transformers.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FdJ3qBkRFcO-",
        "outputId": "be1b1f8d-ce34-483b-e314-0be6ec058f34"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/huggingface/transformers.git\n",
            "  Cloning https://github.com/huggingface/transformers.git to /tmp/pip-req-build-ieii38ax\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/huggingface/transformers.git /tmp/pip-req-build-ieii38ax\n",
            "  Resolved https://github.com/huggingface/transformers.git to commit bab40c6838c97f56022c0f3340b27aff89692b4d\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers==4.53.0.dev0) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers==4.53.0.dev0) (0.31.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers==4.53.0.dev0) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers==4.53.0.dev0) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers==4.53.0.dev0) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers==4.53.0.dev0) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers==4.53.0.dev0) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers==4.53.0.dev0) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers==4.53.0.dev0) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers==4.53.0.dev0) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers==4.53.0.dev0) (2025.3.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers==4.53.0.dev0) (4.13.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.53.0.dev0) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.53.0.dev0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.53.0.dev0) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.53.0.dev0) (2025.4.26)\n",
            "Building wheels for collected packages: transformers\n",
            "  Building wheel for transformers (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for transformers: filename=transformers-4.53.0.dev0-py3-none-any.whl size=11372487 sha256=2ccda5814ba9562d3d52ca8d1983079e7a1908adb12bc5b7f60500d89a047d4c\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-j7o0xjjc/wheels/32/4b/78/f195c684dd3a9ed21f3b39fe8f85b48df7918581b6437be143\n",
            "Successfully built transformers\n",
            "Installing collected packages: transformers\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.52.2\n",
            "    Uninstalling transformers-4.52.2:\n",
            "      Successfully uninstalled transformers-4.52.2\n",
            "Successfully installed transformers-4.53.0.dev0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "l1XJEmWNbkip"
      },
      "outputs": [],
      "source": [
        "!pip install -q datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load CSV\n",
        "\n",
        "We'll download a multi-label image classification dataset from here: https://www.kaggle.com/datasets/meherunnesashraboni/multi-label-image-classification-dataset?resource=download.\n",
        "\n",
        "Next, let's read the associated csv as a Pandas dataframe. Each row consists of a training example, containing the filename of an image and the corresponding one-hot encoded labels. This is exactly the format we need for multi-label classification :)"
      ],
      "metadata": {
        "id": "VT9hYUAu1ZTf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv(\"/content/drive/MyDrive/ConvNext/Tutorial notebooks/Data/multilabel_modified/multilabel_classification(2).csv\")\n",
        "df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 351
        },
        "id": "1rKRkhA21bRP",
        "outputId": "99d99301-73ea-4daf-8829-ce929a68a5dd"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/drive/MyDrive/ConvNext/Tutorial notebooks/Data/multilabel_modified/multilabel_classification(2).csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-451146b192c5>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/drive/MyDrive/ConvNext/Tutorial notebooks/Data/multilabel_modified/multilabel_classification(2).csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/MyDrive/ConvNext/Tutorial notebooks/Data/multilabel_modified/multilabel_classification(2).csv'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's check one example:"
      ],
      "metadata": {
        "id": "rbzrv4mnFExl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.iloc[62].values"
      ],
      "metadata": {
        "id": "gH5pmwem4b1j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.iloc[0][2:].values"
      ],
      "metadata": {
        "id": "d8CZSAb812Tt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "labels = list(df.columns)[2:]\n",
        "print(labels)"
      ],
      "metadata": {
        "id": "rm1peDNr30IA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We create an id2label dictionary which maps integers to strings."
      ],
      "metadata": {
        "id": "SKWGvJzHGKii"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "id2label = {id: label for id, label in enumerate(labels)}\n",
        "print(id2label)"
      ],
      "metadata": {
        "id": "_tc6zHtN34oX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load model and image processor\n",
        "\n",
        "Next, let's instantiate a pre-trained model from the hub, and add a randomly initialized classification head on top.\n",
        "\n",
        "* At the time of writing this notebook, [SigLIP](https://huggingface.co/docs/transformers/en/model_doc/siglip) is one of the best pre-trained image encoders, hence we'll use a checkpoint of it. It has been pre-trained on a lot of image-text pairs in a contrastive fashion similar to [CLIP](https://huggingface.co/docs/transformers/en/model_doc/clip). Note that you can specify any image classifier supported [here](https://huggingface.co/docs/transformers/model_doc/auto#transformers.AutoModelForImageClassification.from_pretrained)!\n",
        "\n",
        "* We also specify the `problem_type` to be \"multi_label_classification\". This makes sure that the appropriate loss function will be used when passing labels to the model (which in the case of multi-label classification is the [binary cross-entropy loss with logits](https://pytorch.org/docs/stable/generated/torch.nn.BCEWithLogitsLoss.html)).\n",
        "* Finally we also pass the id2label mapping, which will make it easy to map integer labels back to text at inference time."
      ],
      "metadata": {
        "id": "uvIQpnXGcaec"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoImageProcessor, AutoModelForImageClassification\n",
        "\n",
        "model_id = \"google/siglip-so400m-patch14-384\"\n",
        "\n",
        "processor = AutoImageProcessor.from_pretrained(model_id)\n",
        "model = AutoModelForImageClassification.from_pretrained(model_id, problem_type=\"multi_label_classification\", id2label=id2label)"
      ],
      "metadata": {
        "id": "UWaXm8FUcaEY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create PyTorch Dataset\n",
        "\n",
        "Next we'll create a regular PyTorch dataset, which prepares the data for the model. Each training example consists of 2 things:\n",
        "* pixel_values, which is the image prepared in the format that the model expects\n",
        "* labels, the corresponding multiple labels, as a one-hot encoded vector."
      ],
      "metadata": {
        "id": "_lwH2IP_ciYj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset\n",
        "import torch\n",
        "from PIL import Image\n",
        "import os\n",
        "import numpy as np\n",
        "\n",
        "class MultiLabelDataset(Dataset):\n",
        "  def __init__(self, root, df, transform):\n",
        "    self.root = root\n",
        "    self.df = df\n",
        "    self.transform = transform\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    item = self.df.iloc[idx]\n",
        "    # get image\n",
        "    image_path = os.path.join(self.root, item[\"Image_Name\"])\n",
        "    image = Image.open(image_path).convert(\"RGB\")\n",
        "\n",
        "    # prepare image for the model\n",
        "    pixel_values = self.transform(image)\n",
        "\n",
        "    # get labels\n",
        "    labels = item[2:].values.astype(np.float32)\n",
        "\n",
        "    # turn into PyTorch tensor\n",
        "    labels = torch.from_numpy(labels)\n",
        "\n",
        "    return pixel_values, labels\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.df)"
      ],
      "metadata": {
        "id": "H_sgmqm7ciCL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To prepare the images for the model, we'll use the [Torchvision](https://pytorch.org/vision/stable/index.html) package which provides several nice image transformations which we can use. Of course, feel free to use another library here, like Albumentations.\n",
        "\n",
        "The most important thing to keep in mind here is that images need to be resized to the size that the model expects (which in this case is 384), and that we normalize the color channels with the appropriate mean and standard deviation. These properties can all be found from the image processor, which is a minimal object typically used for inference only."
      ],
      "metadata": {
        "id": "3mUbNCuLHUcp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision.transforms import Compose, Resize, ToTensor, Normalize\n",
        "\n",
        "# get appropriate size, mean and std based on the image processor\n",
        "size = processor.size[\"height\"]\n",
        "mean = processor.image_mean\n",
        "std = processor.image_std\n",
        "\n",
        "transform = Compose([\n",
        "    Resize((size, size)),\n",
        "    ToTensor(),\n",
        "    Normalize(mean=mean, std=std),\n",
        "])\n",
        "\n",
        "train_dataset = MultiLabelDataset(root=\"/content/drive/MyDrive/ConvNext/Tutorial notebooks/Data/multilabel_modified/images\",\n",
        "                                  df=df, transform=transform)"
      ],
      "metadata": {
        "id": "Sk3Htexcqwrj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pixel_values, labels = train_dataset[63]\n",
        "print(pixel_values.shape)"
      ],
      "metadata": {
        "id": "lkJDLHiA3HVh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's verify the preprocessed image by \"denormalizing\" the pixel values:"
      ],
      "metadata": {
        "id": "NKU_ZBSDH6_9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "unnormalized_image = (pixel_values.numpy() * np.array(std)[:, None, None]) + np.array(mean)[:, None, None]\n",
        "unnormalized_image = (unnormalized_image * 255).astype(np.uint8)\n",
        "unnormalized_image = np.moveaxis(unnormalized_image, 0, -1)\n",
        "Image.fromarray(unnormalized_image)"
      ],
      "metadata": {
        "id": "7TD9cX-vH9jv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's verify the corresponding one-hot encoded vector of labels applicable to this image:"
      ],
      "metadata": {
        "id": "lv9cKGOfISrE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "labels"
      ],
      "metadata": {
        "id": "l35iXk8t4kFi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.nonzero(labels).squeeze().tolist()"
      ],
      "metadata": {
        "id": "M0oj0-1K5RC7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "[id2label[label] for label in torch.nonzero(labels).squeeze().tolist()]"
      ],
      "metadata": {
        "id": "RqIpCCyz4mrb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create PyTorch DataLoader\n",
        "\n",
        "Cool! Next we can create a corresponding [PyTorch DataLoader](https://pytorch.org/tutorials/beginner/basics/data_tutorial.html), in order to get batches of training examples (as neural networks are typically trained on batches of data using stochastic gradient descent = SGD)."
      ],
      "metadata": {
        "id": "JjGsB4ygjOSQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "def collate_fn(batch):\n",
        "    data = torch.stack([item[0] for item in batch])\n",
        "    target = torch.stack([item[1] for item in batch])\n",
        "    return data, target\n",
        "\n",
        "train_dataloader = DataLoader(train_dataset, collate_fn=collate_fn, batch_size=2, shuffle=True)"
      ],
      "metadata": {
        "id": "eoEKM-Z-jOLh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch = next(iter(train_dataloader))\n",
        "print(batch)"
      ],
      "metadata": {
        "id": "gt3JWSsj5ocg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(batch[0].shape)\n",
        "print(batch[1].shape)"
      ],
      "metadata": {
        "id": "I1VgCaZ2JQCp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Verify initial loss"
      ],
      "metadata": {
        "id": "oTW-m6-E03SG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "outputs = model(pixel_values=batch[0].to(device), labels=batch[1].to(device))"
      ],
      "metadata": {
        "id": "PZiJfqqo04hX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "outputs.loss"
      ],
      "metadata": {
        "id": "vX6d-xc11Biz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train the model\n",
        "\n",
        "It's time to train the model! We'll train in regular PyTorch fashion here, but feel free to upgrade to leverage ðŸ¤— [Accelerate](https://huggingface.co/docs/accelerate/index) (very useful for distributed training with minimal code changes), or leverage the ðŸ¤— [Trainer](https://huggingface.co/docs/transformers/main_classes/trainer) class which handles a lot of the logic we're defining here for you (like creating dataloaders).\n",
        "\n",
        "NOTE: this notebook is only made for demo purposes. There's a lot to improve/tweak here:\n",
        "- learning rate\n",
        "- number of epochs\n",
        "- optimizer\n",
        "- gradient accumulation, gradient checkpointing, Flash Attention can be leveraged to speed up training\n",
        "- mixed precision training (bfloat16) etc.\n",
        "\n",
        "If you want a lot of this to be easily configurable, I'd recommend using the [Trainer](https://huggingface.co/docs/transformers/main_classes/trainer) API. For example usage, I refer to the [example script](https://github.com/huggingface/transformers/blob/main/examples/pytorch/image-classification/run_image_classification.py)."
      ],
      "metadata": {
        "id": "U8WbARMlInsY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# handy utility I found at https://github.com/wenwei202/pytorch-examples/blob/ecbb7beb0fac13133c0b09ef980caf002969d315/imagenet/main.py#L296\n",
        "class AverageMeter(object):\n",
        "    \"\"\"Computes and stores the average and current value\"\"\"\n",
        "    def __init__(self):\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.val = 0\n",
        "        self.avg = 0\n",
        "        self.sum = 0\n",
        "        self.count = 0\n",
        "\n",
        "    def update(self, val, n=1):\n",
        "        self.val = val\n",
        "        self.sum += val * n\n",
        "        self.count += n\n",
        "        self.avg = self.sum / self.count"
      ],
      "metadata": {
        "id": "LHZdjLbzJ61l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.optim import AdamW\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "# move model to GPU\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
        "\n",
        "losses = AverageMeter()\n",
        "\n",
        "model.train()\n",
        "for epoch in range(10):  # loop over the dataset multiple times\n",
        "    for idx, batch in enumerate(tqdm(train_dataloader)):\n",
        "        # get the inputs;\n",
        "        pixel_values, labels = batch\n",
        "\n",
        "        # zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # forward pass\n",
        "        outputs = model(\n",
        "            pixel_values=pixel_values.to(device),\n",
        "            labels=labels.to(device),\n",
        "        )\n",
        "\n",
        "        # calculate gradients\n",
        "        loss = outputs.loss\n",
        "        losses.update(loss.item(), pixel_values.size(0))\n",
        "        loss.backward()\n",
        "\n",
        "        # optimization step\n",
        "        optimizer.step()\n",
        "\n",
        "        if idx % 100 == 0:\n",
        "            print('Epoch: [{0}]\\t'\n",
        "                  'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'.format(\n",
        "                   epoch, loss=losses,))"
      ],
      "metadata": {
        "id": "lrGvRKRs5qN9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Inference\n",
        "\n",
        "Let's showcase inference on a new image."
      ],
      "metadata": {
        "id": "8hqtWZSp1wvy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# load image to test on\n",
        "image = Image.open(\"/content/drive/MyDrive/ConvNext/Tutorial notebooks/Data/multilabel_modified/images/image6179.jpg\")\n",
        "image"
      ],
      "metadata": {
        "id": "xDYmSEVj13fd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "\n",
        "# prepare image for the model\n",
        "pixel_values = processor(image, return_tensors=\"pt\").pixel_values.to(device)\n",
        "\n",
        "# forward pass\n",
        "with torch.no_grad():\n",
        "  outputs = model(pixel_values)\n",
        "  logits = outputs.logits"
      ],
      "metadata": {
        "id": "JG7QLSKpKqKm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note: as we used the [BCEWithLogitsLoss](https://pytorch.org/docs/stable/generated/torch.nn.BCEWithLogitsLoss.html) during training (which applies a sigmoid on the logits before calculating a loss), we need to apply sigmoid to the logits here as well. This turns them into individual probabilities."
      ],
      "metadata": {
        "id": "xyGj8ye05BWo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# turn into probabilities by applying sigmoid\n",
        "sigmoid = torch.nn.Sigmoid()\n",
        "probs = sigmoid(logits.squeeze().cpu())\n",
        "\n",
        "# select the probabilities > a certain threshold (e.g. 50%) as predicted\n",
        "predictions = np.zeros(probs.shape)\n",
        "predictions[np.where(probs >= 0.5)] = 1 # turn predicted id's into actual label names\n",
        "predicted_labels = [id2label[idx] for idx, label in enumerate(predictions) if label == 1.0]\n",
        "print(predicted_labels)"
      ],
      "metadata": {
        "id": "Vx2KR8sO34cl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "I barely trained the model on only 50 examples for debugging purposes, and it looks like training is working (model is predicting \"truck\" here instead of \"bus\" but would require more training)."
      ],
      "metadata": {
        "id": "97k9optUHD-G"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PGElF9o04NEv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}